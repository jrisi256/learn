---
title: 'Fundamentals of Probability with Stochastic Processes: 4th Edition'
output: html_document
---

## Book Info

* Author: Saeed Ghahramani
* Title: Fundamentals of Probability with Stochastic Processes: 4th Edition
* Publication Year: 2019

### Chapter 1

#### Review of Set Theory

For all following notation, the sample space is defined by $S$.

* **Subset**: Event $E$ is said to be a subset of event $F$ if whenever $E$ occurs, $F$ also occurs. This means all sample points/elements of $E$ are contained within $F$.
  * **Notation**: $E \subseteq F$
* **Equality**: Events $E$ and $F$ are said to be equal if the occurrence of $E$ implies the occurrence of $F$ and vice versa.
  * **Notation**: $E \subseteq F$ and $F \subseteq E$ which means $E = F$.
* **Intersection**: An event is called the intersection of two events, $E$ and $F$, if it occurs only whenever $E$ and $F$ occur simultaneously.
  * **Notation**: $E \cap F$ or $EF$
* **Union**: An event is called the union of two events, $E$ and $F$, if it occurs whenever at least one of them occurs.
  * **Notation**: $E \cup B$
* **Complement**: An event is called the complement of an event, $E$, if it only occurs whenever $E$ does not occur.
  * **Notation**: $E^{c}$
* **Difference**: An event is called the difference of two events, $E$ and $F$, if it occurs whenever $E$ occurs but $F$ does not.
  * **Notation**; $E - F$ and $E^{c} = S - E$ and $E - F = E \cap F^{c}$
* **Impossibility**: An event is impossible if there is certainty in its non-occurrence.
  * **Notation**: $\varnothing$ and $S^{c}$
* **Mutually Exclusive**: If the joint occurrence of two events $E$ and $F$ is impossible, we say that $E$ and $F$ are mutually exclusive. Thus $E$ and $F$ are mutually exclusive if the occurrence of $E$ precludes the occurrence of $F$ and vice versa.
  * **Notation**: $E \cap F = \varnothing$

Sets and set operations also follow certain laws from arithmetic:

* **Commutative**: $E \cup F = F \cup E$ and $EF = FE$
* **Associative**: $E \cup (F \cup G) = (E \cup F) \cup G$ and $E(FG) = (EF)G$
* **Distributive**: $(EF) \cup H = (E \cup H)(F \cup H)$ and $(E \cup F)H = (EH) \cup (FH)$
* **De Morgan's 1st Law**: $(E \cup F)^{c} = E^{c}F^{c}$
* **De Morgan's 2nd Law**: $(EF)^{c} = E^{c} \cup F^{c}$

#### Review of Probability Axioms (statements which don't need to be proven true)

Let $S$ be the sample space of a random phenomenon. Suppose that each event $A_{i}$ of $S$ has a number denoted by $P(A)$. If $P$ satisfies the following axioms then it is to be called a probability.

* **Axiom 1**: $P(A) \ge 0$
* **Axiom 2**: $P(S) = 1$
* **Axiom 3**: If $\{A₁, A₂, A₃, ...\}$ is a sequence of mutually exclusive events then the probability of the union of these events happening is equal to the summation of the probabilities for each of these events.
  * Also known as the axiom of countable additivity.
  * This also implies you can add the probability of mutually exclusive events to get the probability of the union of those events happening.
  * In English, this is usually denoted with **or**. For example, what is the probability of rolling a 1 **OR** a 2 **OR** a 3? You can simply add the individual probabilities to find out. You **CANNOT** do this for events which are not mutually exclusive.
* **Theorem 1.6**: Yes I know my theorems are out of order so sue me. It seemed appropriate to place here. This theorem states that: $P(A \cup B) = P(A) + P(B) - P(AB)$.
  * For mutually exclusive events, $P(AB)$ is 0 since they never intersect.
  * For non-mutually exclusive events, we have to subtract off the intersection because otherwise we are double counting.
* From **Axiom 3** and **Theorem 1.6**, we can generalize to find the $P(A_{1} \cup A_{2} \cup ... \cup A_{n})$ i.e. the probability that at least one of the events occur. **Inclusion-Exclusion Principle**: To calculate $P(A_{1} \cup A_{2} \cup ... \cup A_{n})$first find all of the possible intersections of these events and calculate those probabilities. Then add the probabilities of those intersections that are formed by an odd number of events and subtract the probabilities of those formed of an even number of events. The intuition being we have to add back in those intersections which we were subtracting out (we don't want to double under-count).

Some theorems we can deduce from these axioms:

* **Theorem 1.1**: $P(\varnothing) = 0$
* **Theorem 1.2**: If $\{A_{1}, A_{2}, A_{3}, ..., A_{n}\}$ is a sequence of mutually exclusive events, then the probability of the union of these events happening is equal to the summation of the probabilities for each of these events.
  * Axiom 3 is stated for a countably infinite collection of mutually exclusive events. Theorem 1.2 demonstrates this property holds for a finite collection of mutually exclusive events as well.

We can finally come to some definition of probability. If we have a sample space $S$ then we also have $\mathcal{P}(S)$ which is the set of all subsets of $S$ (aka the **power set**). The aim of probability theory is to associate a number between 0 and 1 to every subset of the sample space. Probability is thus a function $P$ whose domain is $\mathcal{P}(S)$ and whose range is $[0, 1]$. Since the domain of the $P$ (probability function) is a collection of sets, it is called a **set function**.

In conclusion, probability is a **real-valued**, **non-negative**, **countably additive**, **set function**.

* **Theorem 1.3**: Let $S$ be the sample space. If $S$ has $N$ points that are all equally likely to occur then for any event $A$ of $S$: $P(A) = \frac{N(A)}{N}$.
* **Theorem 1.4**: For any event $A$, $P(A^{c}) = 1 - P(A)$
* **Theorem 1.5**: If $A \subseteq B$ then $P(B - A) = P(B) - P(A)$.
  * **Corollary**: If $A \subseteq B$ then $P(A) \le P(B)$.
  * **Interpretation**: It is more likely for a computer to have at least one defect versus having exactly one defect.
  * **Note**: The condition of $A \subseteq B$ is very important. For example in rolling a single, fair die let's say we define:
  
  \begin{align*}
  &B = \{1, 2\} \\
  &A = \{3, 4, 5\} \\
  &B - A = \{1,2\} \\
  &P(B - A) = \frac{1}{3} \\
  &P(B) = \frac{1}{3} \\
  &P(A) = \frac{1}{2} \\
  &P(B) - P(A) \neq P(B - A)
  \end{align*}

* **Definition 1.2**: A point is said to be randomly selected from an interval $(a, b)$ if any two sub-intervals of $(a, b)$ that have the same length are equally likely to include the point. The probability associated with the event that the sub-interval $(c, d)$ contains the point is defined to be $\frac{d - c}{b - a}$.

The book has a very interesting explanation and investigation of this definition. The model to keep in mind is you have a box with infinitely many balls with each ball given exactly one number from $(a, b)$ and each number from $(a, b)$ can be found on only one ball. The balls are completely mixed up so then in a random selection every ball has the same chance as being drawn as any other ball.

#### Simulation

As we know, choosing a random number from a given interval is, in practice, impossible. To perform simulation, we must use pseudo-random numbers instead. To generate $n$ pseudo-random numbers from a uniform distribution on $(a, b)$, we take an initial value $x_{0} \in (a, b)$ which is called the **seed**. We then construct a function $f$ so that the sequence $\{x_{1}, x_{2}, ..., x_{n}\} \subset (a, b)$ obtained recursively from $x_{i + 1} = f(x_{i}), 0 \le i \le n - 1$ satisfies certain statistical tests for randomness.

Because the numbers generated are rounded to a certain number of decimal places, $f$ can only generate a finite number of pseudo-random numbers. This implies eventually some xⱼ will be generated a second time. From that point on, the same sequence of numbers that appeared after xⱼ's first appearance will reappear. Beyond that point, numbers are no longer sufficiently random. $f$ is constructed in such a way so as to postpone this for as long as possible.

So it is surprising that there are deterministic real-valued functions $f$ that for each $i$ generate an $x_{i + 1}$ that is completely determined by $x_{i}$ and yet the sequence $\{x_{1}, x_{2}, ..., x_{n}\}$ will still pass certain statistical tests of randomness.

### Chapter 2

* **Theorem 2.1 (The Counting Principle)**: If the set $E$ contains $n$ elements and the set $F$ contains $m$ elements then there are $n * m$ ways in which we can choose, first, an element of $E$ then an element of $F$.

* **Theorem 2.2 (The Generalized Counting Principle)**: Let $E_{1}, E_{2}, ..., E_{k}$ be sets with $n_{1}, n_{2}, ..., n_{k}$ elements, respectively. There are then $n_{1} * n_{2} * ... * n_{k}$ ways in which we can choose, first, an element from $E_{1}$, then an element from $E_{2}$, so on and so forth until we choose a final element from $E_{k}$.

An important concept is drawing cards, balls, or anything **with replacement** or **without replacement**. When something is drawn with replacement, it is replaced. If something is drawn without replacement, it is not replaced.

Let's do some example problems.

* **Example 2.1**: How many outcomes are there if we throw five dice?

```{r}
6 * 6 * 6 * 6 * 6
```

* **Example 2.2**: In tossing four dice, what is the probability at least one is 3? We can find the numerator by finding **1 - the complement** or **1 - the number of outcomes where you get no 3s**.

```{r}
# Denominator
denom <- 6 * 6 * 6 * 6
denom

# Numerator
numer <- 5 * 5 * 5 * 5
numer

# Probability
1 - (numer/denom)
```

Of course we can also do this the hard mode way. We find the *P(all four dice are 3)* + *P(three dice show 3)* + *P(two dice show 3)* + *P(one die shows 3)*. We will see in later chapters how to do this in an easier fashion although it still won't ever be as easy as the above method.

How many outcomes are there which would result in all four dice having a 3? This is easy. There is only **1** way for this to happen.

How many outcomes are there which would result in **exactly** three dice having a 3? Well let's see. There are $1 * 1 * 1 * 5$ ways in which the first three dice could have a 3. Then there are $1 * 1 * 5 * 1$ ways in which the first, second, and fourth die all have 3. Next there are $1 * 5 * 1 * 1$ ways in which the first, third, and fourth die all have 3. Finally we have $5 * 1 * 1 * 1$ ways in which the last three dice have a 3.

We would then repeat these steps for the situation in which there are two dice that have a 3 and then finally one die which has a 3.

```{r}
exact4 <- 1
exact3 <- (1 * 1 * 1 * 5) * 4
exact2 <- (1 * 1 * 5 * 5) * 6
exact1 <- (1 * 5 * 5 * 5) * 4

newNumer <- exact4 + exact3 + exact2 + exact1
newNumer

newNumer / denom
```

* **Example 2.4**: A box contains 7 balls numbered 1 through 7. Three balls are drawn one by one, at random, and with replacement. What is the probability that A) all three outcomes are odd? and B) exactly one outcome is odd?

The exactly one odd outcome is a bit trickier. $4 * 3 * 3$ represents the total number of outcomes where the odd number is drawn on the first ball and the first ball only. $3 * 4 * 3$ represents the total number of outcomes where the odd number is drawn on the second ball and the second ball only. Finally $3 * 3 * 4$ represents the total number of outcomes where the odd number is drawn on the third ball and the third ball only.

```{r}
denom <- 7 * 7 * 7 # Total number of possible ways to draw the balls
denom

numerA <- 4 * 4 * 4 # Total number of possible ways to draw exactly 3 odd balls
numerA

probA <- numerA / denom
probA

numerB <- (4 * 3 * 3) * 3 # Total # of possible ways to draw exactly 1 odd ball
numerB

probB <- numerB / denom
probB
```

* **Example 2.5**: A box contains 7 balls numbered 1 through 7. Three balls are drawn one by one, at random, and without replacement. What is the probability that A) all three outcomes are odd? and B) exactly one outcome is odd?

Again the exactly one odd outcome is a bit trickier. $4 * 3 * 2$ represents the total number of outcomes where the odd number is drawn on the first ball and first ball only and even numbers are drawn on the second two balls. $3 * 4 * 2$ represents the number of outcomes where the odd number is drawn on the second ball and the second ball only and even numbers are drawn on the first and third balls. Finally, $3 * 2 * 4$ represents the number of outcomes where the odd number is drawn on the third ball and the third ball only and even numbers are drawn on the first two balls.

```{r}
denom <- 7 * 6 * 5 # Total number of ways to draw the balls
denom

numerA <- 4 * 3 * 2 # Total number of possible ways to draw exactly 3 odd balls
numerA

probA <- numerA / denom
probA

numerB <- (4 * 3 * 2) * 3 # Total # of possible ways to draw exactly 1 odd ball
numerB

probB <- numerB / denom
probB
```

* **Example 2.6**: I'm doing this problem a bit differently than the book does it because I'm doing it better. Suppose there are $n$ people at a party. If everyone shakes hands with everyone else at the party exactly once, what is the number of handshakes?

    * Two ways of thinking about this problem:
    
        * **Solution 1**: If there are $n$ people at the party, then they shake hands with $n - 1$ people. This means there are $n$ possible people each of which is conducting $n - 1$ handshakes. However we are overcounting because *person A shaking hands with person B* and *person B shaking hands with person A* is technically the same handshake, but we count it twice. We are doing this for every handshake. So we need to divide by 2, and we arrive at our answer: $n * \frac{n - 1}{2}$
        
        * **Solution 2**: Another way to think about this problem is suppose guests arrive one at a time. The first person won't be able to conduct a handshake. The second person arrives and shakes hands with $n - 1$ people. The third person arrives and shakes hands with $n - 2$ people and so on and so forth. This means there will be $(n - 1) + (n - 2) + (n - 3) + ... + 3 + 2 + 1$ handshakes which is the very famous sequence: $n * \frac{n - 1}{2}$
        
Really the famous sequence is $n * \frac{n + 1}{2}$. This is a shortcut for finding the sum of a sequence of numbers $\{1, 2, ..., n\}$. However, if you wanted to find the sum of a sequence of numbers $\{1, 2, ..., n - 1\}$ (which is what we are doing above) then it is $n * \frac{n - 1}{2}$. If we wanted find the sum of ${1, 2, ..., n - 2}$ then it would become $(n - 1) * \frac{n - 2}{2}$. How did we get this derivation? Well let's start at the beginning $n * \frac{n + 1}{2}$ which is equal to $n + (n - 1) + (n - 2) + ... + 3 + 2 + 1$. When we only want to find the sum up to $n - 1$ we need to remove $n$ from the equation so now it is $(n - 1) + (n - 2) + ... + 3 + 2 + 1$. We then also need to remove it from our formula:

$(n * \frac{n+1}{2}) - \frac{2n}{2} = n * \frac{n - 1}{2}$

A similar thing happens when we want to sum only up to $n - 2$:

1. $(n * \frac{n + 1}{2}) - (n + n - 1)$
2. $n + n - 1$ transforms into $\frac{4n - 2}{2}$
3. Our original formulation in step 1 through algebra becomes $\frac{n^{2} - 3n + 2}{2}$
4. We can factor to get: $\frac{(n - 1) * (n - 2)}{2}$

* **Example 2.8**: The birthday problem (which has such a wide variety of applications in so many different fields). What is the probability that at least two students of a class of size $n$ have the same birthday? Assume the birth rates are constant throughout the year (they are not), and each year has 365 days.

In a reductionist sense, this is basically like having $n$ people randomly pull from a bag full of balls labeled from 1 to 365. This becomes very similar to **Example 2.4** as a result. For demonstration purposes, we are going to find the probability associated with $n = 1$ all the way up to $n = 115$.

So to illustrate what's happening, let's start with $n = 23$. We could do this the hard way and find the probability that all 23 match, then that 22 match, then 21, so on and so forth. Let's do this the simpler way though and find the complement which is to say **the probability nobody matches**.

First, the total number of outcomes is the *number of balls in the bag* (days in the year) to the *nth* power. Right? $365 * 365 * 365 * ... 365$ *n times*. Next the total number of outcomes where nobody has the same birthday is going to be $365 * 364 * 363 * 362 * 361 * ... * (365 - n) + 1$. Each subsequent person has one less day to pick from that would ensure they don't match someone else.

```{r}
nrPeople <- 23
totalNrPossibleOutcomes <- 365 ^ nrPeople
complementNrOutcomes <- prod(seq((365 - nrPeople) + 1, 365))

1 - (complementNrOutcomes / totalNrPossibleOutcomes)
```

Let's turn this into a function and graph the results.

```{r}
library(dplyr)
library(purrr)
library(ggplot2)

ProbBdayMatch <- function(nrPeople, nrDays = 365) {
  denom <- nrDays ^ nrPeople
  numer <- prod(seq(nrDays - nrPeople + 1, nrDays))
  return(list(prob = 1 - (numer / denom), nrPeople = nrPeople))
}

probabilities <- map_dfr(seq(1, 115), ProbBdayMatch)
ggplot(probabilities, aes(x = nrPeople, y = prob)) + geom_point() + theme_bw()
```

* **Example 2.9**: Let's ask a slightly different question around the birthday problem. Terra was born on the 4th of July. Assuming birth rates are constant throughout the year and each year has 365 days, how many people must Terra (randomly) meet to have a 50% (or better) chance of meeting at least one person with the same birthday?

Let's reverse engineer this problem to gain some insight into it. Let's say Terra has met 3 people. What is the probability she will have the same birthday with at least one of them? The total number of possible combinations of birthdays these people could have is $365 * 365 * 365$ or $365^{3}$. The total number of possible outcomes where all 3 of them share Terra's birthday is 1 or $1 * 1 * 1$. The total number of possible outcomes where exactly 2 of them share Terra's birthday is $(364 * 1 * 1) + (1 * 364 * 1) + (1 * 1 * 364)$ where each sub-equation represents the first, second, or third person respectively not sharing a birthday with Terra. The total number of possible outcomes where exactly 1 of them shares Terra's birthday is $(364 * 364 * 1) + (364 * 1 * 364) + (1 * 364 * 364)$ where each sub-equation represents a scenario where only the third, second, or first person respectively share a birthday with Terra. We can then add all of these up and divide by the total number of outcomes to find our probability.

The much easier path is to of course just subtract the complement from one. The total number of possible outcomes where each of the three people do not share a birthday with Terra is $364 * 364 * 364$ or $364^{3}$.

```{r}
match3 <- 1
match2 <- 364 * 3
match1 <- (364 ^ 2) * 3
match0 <- 364 ^ 3
atLeast1 <- match3 + match2 + match1
denom <- 365 ^ 3

atLeast1 / denom
1 - (match0 / denom)
```

We can do something similar when Terra meets 4 people.

```{r}
match4 <- 1
match3 <- (364 * 1 * 1 * 1) + (1 * 364 * 1 * 1) + (1 * 1 * 364 * 1) + (1 * 1 * 1 * 364)
match2 <- (364 * 364 * 1 * 1) + (364 * 1 * 364 * 1) + (364 * 1 * 1 * 364) + (1 * 364 * 364 * 1) + (1 * 364 * 1 * 364) + (1 * 1 * 364 * 364)
match1 <- (364 * 364 * 364 * 1) + (364 * 364 * 1 * 364) + (364 * 1 * 364 * 364) + (1 * 364 * 364 * 364)
match0 <- 364 ^ 4
atLeast1 <- match4 + match3 + match2 + match1
denom <- 365 ^ 4

atLeast1 / denom
1 - (match0 / denom)
```

So we could, of course, do the old guess and check to find out when we get to a number that gets us to 50%. This is, of course, not good enough! We can represent this algebraically.

Our probability in question is: $1 - \frac{364^{n}}{365^{n}}$
And we want this probability to be: $\ge 0.5$
Well let's put this together: $1 - \frac{364^{n}}{365^{n}} \ge 0.5$
And solve it algebraically!

\begin{align*}
&1 - \frac{364^{n}}{365^{n}} \ge 0.5 \\
&\frac{364^{n}}{365^{n}} \le 0.5 \\
&\log_{10}(\frac{364^{n}}{365^{n}}) = \log_{10}(0.5) \\
&n * \log_{10}(\frac{364}{365}) = \log_{10}(0.5)\\
&n = \frac{\log_{10}(0.5)}{\log_{10}(\frac{364}{365})} \\
&n \ge 252.65 \\
\end{align*}

We could, of course, choose to use any logarithm we so desired.

Interpreting our results we would expect Terra to have to meet 253 people to have an approximately 50% chance of meeting at least one person who shared their birthday. I *believe* this means you would expect more often than not that in a random draw of 253 people you would find at least one person with the same birthday as you. In other words, in slightly more than 50% of cases you should expect to meet someone with the same birthday as you in a random draw of 253 people. If you meet 100 groups of 253 people, you should expect that in about 50 of those groups you will come across someone with the same birthday as you.

Let's turn this into a function and graph the results.

```{r}
NrPeopleToMeet <- function(probability, nrDays = 365) {
  
  numerator <- 1 - probability
  return(list(nrPeople = log10(numerator) / log10((nrDays - 1) / nrDays),
              prob = probability))
}

probabilities <- seq(0.01, 0.99, 0.01)
people <- map_dfr(probabilities, NrPeopleToMeet)

ggplot(people, aes(x = prob, y = nrPeople)) + geom_point() + theme_bw()
```

These birthday results might seem strange. Why is it that in a room of 23 people there is an approximately 50% chance of at least two people having the same birthday, but I have to meet 253 people before I have a 50% chance of meeting someone with my birthday? Think of the lottery. The chance of *you* winning is very, very, very small. By comparison, the probability that *someone* wins the lottery is quite high. This suggests the probability of coincidences happening when construed of as broadly or generically is quite high.

* **Theorem 2.3**: A set with $n$ elements has $2^{n}$ subsets. In other words, the power set of a set with $n$ elements has $2^{n}$ elements.

We can explore this theorem with an example and understand the intuition behind it. Suppose we are at a vegan pizza restaurant where customers can choose from among 10 toppings: vegan pepperoni, mushrooms, vegan sausage, green peppers, onions, olives, red peppers, pineapple, garlic, and spinach. How many different types of pizzas could you create?

_ _ _ _ _ _ _ _ _ _ Let's have these 10 blank spaces correspond to the 10 possible pizza toppings. And each one can be represented by a 1 (if we want the topping) and a 0 (if we don't want the topping). This means for each possible pizza topping there are two choices that could be made.

 So then that means the total possible number of pizzas that can be created is: $2^{10}$.

#### Permutations

Let's start with permutations. What is a permutation?

* **Definition 2.1**: An ordered arrangement of $r$ objects from a set $A$ (which has $n$ objects) and follows this inequality $0 \lt r \le n$ is called an **r-element permutation** of $A$. We use the notation: $_{n}P_{r}$ to denote picking $r$ elements from a set of $n$ elements.

If for example $A = \{a, b, c, d\}$ then $ab$ is a two-element permutation of $A$, $acd$ is a three-element permutation of $A$, and $abcd$ is a four-element permutation of $A$.

The formula for computing the number of permutations for a set $A$ containing $n$ elements taken $r$ at a time follows the generalized counting principle we established earlier. For our first choice we will have $n$ options, our second choice will have $n - 1$ options, and so on and so forth until we get to the final choice which $n - (r - 1)$ or $n - r + 1$. Putting it all together: $$_{n}P_{r} = n * (n - 1) * (n - 2) * ... * (n - r + 1)$$

An $n$-element permutation of a set with $n$ objects is simply called a permutation. The formula then simply becomes $n!$.

There is a more popular way to show the equation for $_{n}P_{r}$ which is obtained by multiplying both sides by $(n - r)!$.

\begin{align*}
&_{n}P_{r} * (n - r)! = n * (n - 1) * (n- 2) * ... * (n - r + 1) * (n - r)! \\
&_{n}P_{r} * (n - r)! = n * (n - 1) * (n- 2) * ... * (n - r + 1) * (n - r) * (n - r - 1) * ... * 3 * 2 * 1 \\
&_{n}P_{r} * (n - r)! = n! \\
&_{n}P_{r} = \frac{n!}{(n - r)!}
\end{align*}

* **Example 2.13** Three people: Bobby, Smithy, and Jonesy must be scheduled for job interviews. In how many different orders can this be done?

$_{3}P_{3} = 3! = 6$

* **Example 2.14**: Two copies of book A, four copies of book B, three copies of book C, three copies of book D, and five copies of book E must be arranged on a bookshelf, and it is done so randomly. What is the probability that each copy of the same book are next to each other? For example, all copies of book A are next to each other, all copies of book B are next to each other, etc.

Let's think about this. How many possible ways are there to arrange the books? $17!$ since there are 17 books. Now we want to know how many of those possible arrangements have all the copies of the same book next to each other. Let's list some possible permutations that would fit our criteria:

\begin{align*}
&\{a_{1}, a_{2}, b_{1}, b_{2},  b_{3}, b_{4}, c_{1}, c_{2}, c_{3}, d_{1}, d_{2}, d_{3}, e_{1}, e_{2}, e_{3}, e_{4}, e_{5}\} \\
&\{a_{2}, a_{1}, b_{4}, b_{2},  b_{3}, b_{1}, c_{3}, c_{2}, c_{1}, d_{2}, d_{1}, d_{3}, e_{2}, e_{3}, e_{1}, e_{5}, e_{4}\} \\
&\{e_{1}, e_{2}, e_{3}, e_{4}, e_{5}, c_{1}, c_{2}, c_{3}, a_{1}, a_{2}, b_{1}, b_{2}, b_{3}, b_{4}, d_{1}, d_{2}, d_{3}\}
\end{align*}

So each our subsets of the books can be arranged in a variety of different ways.

\begin{align*}
&_{2}P_{2} = 2! = A \\
&_{4}P_{4} = 4! = B \\
&_{3}P_{3} = 3! = C \\
&_{3}P_{3} = 3! = D \\
&_{5}P_{5} = 5! = E
\end{align*}

So then if we do: $2! * 4! * 3! * 3! * 5!$ we will get the number of permutations that can happen when A is first, B is second, C is third, D is fourth, and E is fifth. But then there are $5!$ ways to order A, B, C, D, and E. So the total number of possible ways to order our books to fit our criteria is: $2! * 4! * 3! * 3! * 5! * 5!$

That's a lot of ways! However, it is but a blip in the total number of ways those books can be ordered.

```{r}
denom <- factorial(17)
numer <- 2 * 24 * 6 * 6 * 120 * 120
numer / denom
```

* **Example 2.15**: If five boys and five girls sit on a bench in random order, what is the probability that no children of the same sex sit together?

The total possible number of outcomes is $10!$. If we imagine them sitting on a bench the general order will have to be: $\{b, g, b, g, b, g, b, g, b, g\}$ OR $\{g, b, g, b, g, b, g, b, g, b\}$. This suggests the possible number of ways to order these children to fit our criteria is: $5! * 5! * 2$. Our answer below:

```{r}
denom <- factorial(10)
numer <- 120 * 120 * 2
numer / denom
```

Permutations can get a bit tricky when some of the elements are indistinguishable. For example let's consider the number of permutations of the 8 letters in B E R K E L E Y. It will not be $8!$. What will it be? Let's consider the permutation B Y E R E L E K. What if we labeled this as $B Y E_{1} R E _{2} L E _{3} K$? Well then it might be obvious there are 6 ($3!$) ways we could rearrange this particular permutation which otherwise would look the same. This means the total number of permutations is actually $\frac{8!}{3!}$. This is because we are actually creating too many permutations by a factor of $3!$. For each permutation there are $3!$ other permutations we are considering that should not be considered. This leads us to our new theorem.

* **Theorem 2.4**: If we have the following formulation where $n = n_{1} + n_{2} + ... + n_{k}$. The, the number of distinguishable permutations of $n$ objects of $k$ different types where $n_{1}$ objects are alike, $n_{2}$ objects are alike, so on and so forth, until finally $n_{k}$ objects are alike will be: $$\frac{n!}{n_{1}! * n_{2} * ... * n_{k}!}$$

This means what we've been doing before this has **REALLY** been something like $\frac{4!}{1! * 1! * 1! * 1!}$. The Berkely example is actually $\frac{8!}{3! * 1! * 1! * 1! * 1! * 1!}$.

* **Example 2.16**: How many different 10-letter codes can be made using three A's, four B's, and three C's?

```{r}
factorial(10) / (factorial(3) * factorial(4) * factorial(3))
```

* **Example 2.17**: How many ways can we paint 11 offices so that four of them are green, three yellow, two white, and two pink?

```{r}
factorial(11) / (factorial(4) * factorial(3) * factorial(2) * factorial(2))
```

* **Example 2.18**: What is the probability of obtaining exactly three heads after a coin is flipped three times?

We were doing these types of problems earlier in the chapter! Now we have more knowledge concerning how to tackle this problem a bit more intelligently and elegantly. We need not apply so much brute force. 

1. We know the total number of possible flips is $2^{10}$
2. Let's consider one potential sequence which fits our criteria: $HHHTTTTTTT$.
3. Let's consider another sequence which fits our criteria: $HTHTHTTTTT$
4. We are basically trying to figure out how many different permutations there are which would be: $_{10}P_{3} = \frac{10!}{7!}$
5. **BUT!** We are overcounting by a factor of $3!$. So we will need to divide that out too. $\frac{10!}{7! * 3!}$
6. Then we divide the value in step 5 with the total number of outcomes found in step 1 to arrive at our probability. Let's do some math!

```{r}
denom <- 2 ^ 10
nrPerm <- factorial(10) / factorial(7)
numer <- nrPerm / factorial(3)
numer / denom
```

#### Combinations

Sometimes the order in which the elements are arranged isn't important which we experienced a little bit above. So we use **combinations** instead of permutations to deal with these problems. In other words, $\{1, 2, 3\}$ would be the same as $\{2, 1, 3\}$ for counting purposes.

Let's do an intuitive derivation of the formula for finding the number of combinations. Let $x$ be the number of $r$-element combinations of a set $A$ with $n$ objects. If every permutation of each $r$-element combination is found, then all $r$-element permutations have been found.

To put some numbers to this, $_{6}P_{3} = 120$ where $n = 6, r = 3, x = ?$. We know there are 120 total permutations which can be made when creating 3-element permutations from a set $A$ of size 6. So we need to find $x$.

Let's think about this in a different way. Each $r$-element combination of set $A$ (of which there are an $x$ amount) has $r!$ permutations. So $\{1, 2, 3\}$, which is a combination, has $3! = 6$ ways of being permuted.

So then if all $x$ of the combinations has $r!$ permutations, we'd know the total number of $r$-element permutations. This sounds like a formula to me which we can use to solve for $x$.

\begin{align*}
&_{n}P_{r} = x * r! \\
&\frac{n!}{(n - r)!} = x * r! \\
&\frac{n!}{(n - r)! * r!} = x \\
&_{n}C_{r} = \frac{n!}{(n - r)! * r!}
\end{align*}

**Note on notation** we usually use $\binom{n}{r}$ to denote the number of combinations.

* **Example 2.19**: In how many ways can two math and three biology books be selected from eight math and six biology books?

```{r}
(factorial(8) / (factorial(6) * factorial(2))) * (factorial(6) / (factorial(3) * factorial(3)))
```

* **Example 2.20**: A random sample of 45 instructors from different state universities were selected randomly and asked whether they were happy with their teaching loads. 32 of the responses were negative. What is the probability that Drs. Smithy, Browny, and Jonesy (assuming they were asked) all gave negative responses?

Well the total number of combinations of negative responses is $\binom{45}{32}$. We want to know how many of these combinations contains the doctors mentioned above. We can do this by considering a particular combination $\{p_{1}, p_{2}, ..., p_{32}\}$ and let's say $p_{1} = Smithy, p_{2} = Browny, p_{3} = Jonesy$. Well the other 29 professors can be any combination of the remaining 42 professors. By golly, I think we just happened upon our answer.

```{r}
(factorial(42) / (factorial(29) * factorial(13))) / (factorial(45) / (factorial(32) * factorial(13)))
```

* **Example 2.21**: In a smalltown, 8 of the 25 schoolteachers are pro-life, 11 are pro-choice, and the rest are indifferent. A random sample of five schoolteachers are selected for an interview. What is the probability that a) all of them are pro-life; b) all of them have the same opinion?

1. The total number of possible combinations is $\binom{25}{5}$.
2. The total number of possible combinations where every sample member is pro-life is $\binom{8}{5}$.
3. The total number of possible combinations where every sample member has the same opinion is $\binom{8}{5} + \binom{11}{5} + \binom{6}{5}$. Since they are mutually exclusive, and since we are asking what is the probability this happens **OR** this happens **OR** this happens, we can add them.

```{r}
denom <- factorial(25) / (factorial(5) * factorial(20))
numerA <- factorial(8) / (factorial(5) * factorial(3))
numerB <- numerA + (factorial(11) / (factorial(5) * factorial(6))) + (factorial(6) / factorial(5))

numerA / denom
numerB / denom
```

* **Example 2.22**: In Maryland's lottery, players pick six different integers between 1 and 49 where order is irrelevant. The lottery commission randomly selects six of these as the winning numbers. Participants win the grand prize if all 6 match, second prize if 5 match, and third prize if 4 match. What's the probability a participant wins at least third prize?

1. Total number of combinations is $\binom{49}{6}$.

2. The number of outcomes where somebody randomly guesses all 6 numbers correctly is 1.

3. The number of outcomes where somebody randomly guesses 5 of the numbers correctly... hmmm... this is a bit tricky. I actually got this wrong. Well let's think about this. Suppose the winning combination is $\{1, 2, 3, 4, 5, 6\}$. Let's say you then guessed $\{1, 2, 3, 4, 5, 7\}$. Well hooray you got second place. $\{1, 2, 3, 4, 5, 8\}$ would also have gotten us second prize. Same goes for $\{1, 2, 3, 4, 5, 9\}$. In total there are 43 possible combination of results that could have gotten us second place. But that's not all. Suppose we actually guessed $\{2, 3, 4, 5, 6, 7\}$. Well this is also a valid second-place entry. There are 43 possible combinations of this entry as well. So really there is going to be $\binom{6}{5} = 6$ (i.e. 6 possible ways of choosing 5 out of the 6 lotto numbers correctly) * $\binom{43}{1} = 43$ (i.e. 43 possible ways of choosing 1 out of the 43 remaining non-lotto numbers).

4. The number of outcomes where somebody randomly guesses 4 of the numbers correctly. Well following the logic from above there will be $\binom{6}{4} = 15$ (i.e. 15 possible ways of chossing 4 out of the 6 lotto numbers correctly) * $\binom{43}{2} = 903$ (i.e. 903 possible ways of choosing 2 out of the remaining 43 non-lotto numbers).

```{r}
denom <- factorial(49) / (factorial(6) * factorial(43))
numer <- 1 + (6 * 43) + (15 * 903)
numer / denom
```

* **Example 2.23**: Seven cards are drawn at random without replacement from a 52-card deck. What is the probability that at least one of the cards is a king?

1. Total number of possible seven card hands: $\binom{52}{7}$
2. Total number of possible seven card hands with 4 kings: $\binom{4}{4} * \binom{48}{3}$
3. Total number of possible seven card hands with 3 kings: $\binom{4}{3} * \binom{48}{4}$
4. Total number of possible seven card hands with 2 kings: $\binom{4}{2} * \binom{48}{5}$
5. Total number of possible seven card hands with 1 king: $\binom{4}{1} * \binom{48}{6}$
6. Total number of possible seven card hands with 0 kings: $\binom{4}{0} * \binom{48}{7}$

```{r}
denom <- factorial(52) / (factorial(7) * factorial(45))
kings4 <- factorial(48) / (factorial(3) * factorial(45))
kings3 <- 4 * (factorial(48) / (factorial(4) * factorial(44)))
kings2 <- 6 * (factorial(48) / (factorial(5) * factorial(43)))
kings1 <- 4 * (factorial(48) / (factorial(6) * factorial(42)))
kings0 <- factorial(48) / (factorial(7) * factorial(41))

atleast1 <- kings4 + kings3 + kings2 + kings1
atleast1 / denom
1 - (kings0 / denom)
```

* **Example 2.24**: What is the probability that a poker hand is a full house? A poker hand consists of 5 randomly selected cards from a 52-card deck. A full house is when three cards are on denomination and two cards are of another denomination e.g. three queens and two 4's.

1. Total number of possible five card hands: $\binom{52}{5}$
2. 

```{r}

```
